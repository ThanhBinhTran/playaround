{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run with Python 3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import argparse\n",
    "import pdb\n",
    "import threading\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def site_open(site):\n",
    "    '''Makes connection and opens up target website. Returns a website object.'''\n",
    "    try:\n",
    "        #sets up request object\n",
    "        req = urllib.request.Request(site)\n",
    "\n",
    "        #adds User-Agent info to request object\n",
    "        req.add_header(\"User-Agent\",\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) \\\n",
    "         AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.46 Safari/536.5\")\n",
    "\n",
    "        #opens up site\n",
    "        website = urllib.request.urlopen(req)\n",
    "\n",
    "        return website\n",
    "    except urllib.request.URLError:\n",
    "#            print('Could not connect to '+ site + '!')\n",
    "           pass\n",
    "    \n",
    "def soup_site(site):\n",
    "    '''opens site and turns it into a format to easily parse the DOM. Returns a Soup Object'''\n",
    "    try:\n",
    "        website = site_open(site)\n",
    "#         return BeautifulSoup(website, \"html5lib\")\n",
    "        return BeautifulSoup(website, \"lxml\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_by_year(year):\n",
    "    ''' get link url by year and its name'''\n",
    "    venues_lists = [\n",
    "        (\"https://dblp.org/db/conf/ccs/ccs{0}.html\".format(year), \"ccs{0}.csv\".format(year)),\n",
    "        (\"https://dblp.org/db/conf/uss/uss{0}.html\".format(year), \"uss{0}.csv\".format(year)),\n",
    "        (\"https://dblp.org/db/conf/sp/sp{0}.html\".format(year), \"sp{0}.csv\".format(year)),\n",
    "        (\"https://dblp.org/db/conf/ndss/ndss{0}.html\".format(year), \"ndss{0}.csv\".format(year))\n",
    "    ]\n",
    "\n",
    "    #print (venues_lists)\n",
    "    return venues_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "1     http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "2     http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "3     http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "4     http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "5     http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "6     http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "7     http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "8     http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "9     http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "10    http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "11    http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "12    http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "13    http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "14    http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "15    http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "16    http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "17    http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "18    http://capnuoctrungan.vn/Main/Home/TraCuuTienN...\n",
       "dtype: object"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_ID():\n",
    "    partIDs = range (1, 20, 1)\n",
    "    fullIDs =[]\n",
    "    for item in partIDs:\n",
    "        fullIDs.append( \"2209337{0:04d}\".format(item))\n",
    "        \n",
    "    return pd.Series(fullIDs)\n",
    "\n",
    "IDs = generate_ID()\n",
    "\n",
    "IDs\n",
    "links = \"http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=\" + IDs;\n",
    "\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_from_link(link):\n",
    "    soup = soup_site(link)\n",
    "    data =[]\n",
    "    # check if content is present, if yes then parse and get info\n",
    "    if soup != -1:\n",
    "        information = soup.findAll(\"div\", {\"class\" : \"info_label\"})\n",
    "\n",
    "        for info in information:\n",
    "            field = info.label  # get all contents of <table> tag\n",
    "            data_temp = info.b       # get all contents of <b> tag\n",
    "            data_temp = data_temp.string  # get payload of contents\n",
    "            #print (field.string, data_temp)\n",
    "            data.append(data_temp)\n",
    "        print( data)\n",
    "    else:\n",
    "        print (\"Not found {0}\". format(link))\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370001\n",
      " Số Danh Bộ:   22093370001\n",
      " Tên Khách Hàng:   NGUYEN THI NGOC HA\n",
      " Số điện thoại:   \n",
      " Địa Chỉ:   749 NGUYEN VAN QUA\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370001', ' NGUYEN THI NGOC HA', ' ', ' 749 NGUYEN VAN QUA', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370002\n",
      " Số Danh Bộ:   22093370002\n",
      " Tên Khách Hàng:   DUONG NGOC ANH\n",
      " Số điện thoại:   \n",
      " Địa Chỉ:   B112 NGUYEN VAN QUA\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370002', ' DUONG NGOC ANH', ' ', ' B112 NGUYEN VAN QUA', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370003\n",
      " Số Danh Bộ:   22093370003\n",
      " Tên Khách Hàng:   LE VAN HON\n",
      " Số điện thoại:   0905290928\n",
      " Địa Chỉ:   B111 NGUYEN VAN QUA\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370003', ' LE VAN HON', ' 0905290928', ' B111 NGUYEN VAN QUA', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370004\n",
      " Số Danh Bộ:   22093370004\n",
      " Tên Khách Hàng:   NGUYEN VAN DUNG(DD 3 HO)\n",
      " Số điện thoại:   \n",
      " Địa Chỉ:   779 NG V QUA-DHT\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370004', ' NGUYEN VAN DUNG(DD 3 HO)', ' ', ' 779 NG V QUA-DHT', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370005\n",
      " Số Danh Bộ:   22093370005\n",
      " Tên Khách Hàng:   AN THI LOI\n",
      " Số điện thoại:   0903093547\n",
      " Địa Chỉ:   70/3 NGUYEN VAN QUA\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370005', ' AN THI LOI', ' 0903093547', ' 70/3 NGUYEN VAN QUA', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370006\n",
      " Số Danh Bộ:   22093370006\n",
      " Tên Khách Hàng:   DANG THI SEN\n",
      " Số điện thoại:   0334833085\n",
      " Địa Chỉ:   1/124 \n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370006', ' DANG THI SEN', ' 0334833085', ' 1/124 ', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370007\n",
      " Số Danh Bộ:   22093370007\n",
      " Tên Khách Hàng:   NGUYEN THAT\n",
      " Số điện thoại:   0903312337\n",
      " Địa Chỉ:   346 NGUYEN VAN QUA\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370007', ' NGUYEN THAT', ' 0903312337', ' 346 NGUYEN VAN QUA', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370008\n",
      " Số Danh Bộ:   22093370008\n",
      " Tên Khách Hàng:   LAM THANH HUNG\n",
      " Số điện thoại:   \n",
      " Địa Chỉ:   A29 BIS CHO CAU-P.DHT\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370008', ' LAM THANH HUNG', ' ', ' A29 BIS CHO CAU-P.DHT', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370009\n",
      " Số Danh Bộ:   22093370009\n",
      " Tên Khách Hàng:   TO VAN OI\n",
      " Số điện thoại:   0838916124\n",
      " Địa Chỉ:   C98-NG V QUA CC AN SUONG\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370009', ' TO VAN OI', ' 0838916124', ' C98-NG V QUA CC AN SUONG', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370010\n",
      " Số Danh Bộ:   22093370010\n",
      " Tên Khách Hàng:   BAN QUAN TRI CHUNG CU AN SUONG\n",
      " Số điện thoại:   0905009244\n",
      " Địa Chỉ:   LO A1 C/C AN SUONG\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370010', ' BAN QUAN TRI CHUNG CU AN SUONG', ' 0905009244', ' LO A1 C/C AN SUONG', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370011\n",
      " Số Danh Bộ:   22093370011\n",
      " Tên Khách Hàng:   VO THI BA (DD 02 HO)\n",
      " Số điện thoại:   0906666186\n",
      " Địa Chỉ:   940-NG V QUA P.DONG HUNG THUAN 12\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370011', ' VO THI BA (DD 02 HO)', ' 0906666186', ' 940-NG V QUA P.DONG HUNG THUAN 12', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370012\n",
      " Số Danh Bộ:   22093370012\n",
      " Tên Khách Hàng:   VO THI LY EM\n",
      " Số điện thoại:   \n",
      " Địa Chỉ:   14/3B KP1-P.DONG HUNG THUAN\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370012', ' VO THI LY EM', ' ', ' 14/3B KP1-P.DONG HUNG THUAN', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370013\n",
      " Số Danh Bộ:   22093370013\n",
      " Tên Khách Hàng:   TRAN NGOC DUNG\n",
      " Số điện thoại:   0935881838\n",
      " Địa Chỉ:   820 NG V QUA-P.D H THUAN 12\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370013', ' TRAN NGOC DUNG', ' 0935881838', ' 820 NG V QUA-P.D H THUAN 12', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370014\n",
      " Số Danh Bộ:   22093370014\n",
      " Tên Khách Hàng:   LAM VAN DON\n",
      " Số điện thoại:   \n",
      " Địa Chỉ:   A29 NGUYEN VAN QUA\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370014', ' LAM VAN DON', ' ', ' A29 NGUYEN VAN QUA', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370015\n",
      " Số Danh Bộ:   22093370015\n",
      " Tên Khách Hàng:   DINH VAN HIEU\n",
      " Số điện thoại:   0909684678\n",
      " Địa Chỉ:   344 NG V QUA-P.D H THUAN 12\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370015', ' DINH VAN HIEU', ' 0909684678', ' 344 NG V QUA-P.D H THUAN 12', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370016\n",
      " Số Danh Bộ:   22093370016\n",
      " Tên Khách Hàng:   NGUYEN NGOC HUONG (DD 030\n",
      " Số điện thoại:   0933467179\n",
      " Địa Chỉ:   1/135 P.DONG HUNG THUAN\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370016', ' NGUYEN NGOC HUONG (DD 030', ' 0933467179', ' 1/135 P.DONG HUNG THUAN', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370017\n",
      " Số Danh Bộ:   22093370017\n",
      " Tên Khách Hàng:   TRAN VAN BI\n",
      " Số điện thoại:   0979835576\n",
      " Địa Chỉ:   1/136 KP5-P.DONG HUNG THUAN\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370017', ' TRAN VAN BI', ' 0979835576', ' 1/136 KP5-P.DONG HUNG THUAN', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370018\n",
      " Số Danh Bộ:   22093370018\n",
      " Tên Khách Hàng:   TRUONG NGOC DUNG\n",
      " Số điện thoại:   \n",
      " Địa Chỉ:   1/138 KP5-P.DONG HUNG THUAN\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370018', ' TRUONG NGOC DUNG', ' ', ' 1/138 KP5-P.DONG HUNG THUAN', ' không hộp bảo vệ']\n",
      "http://capnuoctrungan.vn/Main/Home/TraCuuTienNuoc?danhbo=22093370019\n",
      " Số Danh Bộ:   22093370019\n",
      " Tên Khách Hàng:   TRAN THANH VIET\n",
      " Số điện thoại:   0973524646\n",
      " Địa Chỉ:   1/142 KP5-P.DONG HUNG THUAN\n",
      " Tình trạng đồng hồ:   không hộp bảo vệ\n",
      "[' 22093370019', ' TRAN THANH VIET', ' 0973524646', ' 1/142 KP5-P.DONG HUNG THUAN', ' không hộp bảo vệ']\n"
     ]
    }
   ],
   "source": [
    "# write to csv file\n",
    "import csv\n",
    "f = open(\"water.csv\", 'w', newline='', encoding=\"utf-8\")\n",
    "writer = csv.writer(f, delimiter=\",\")\n",
    "writer.writerow([\"Số Danh Bộ\", \"Tên Khách Hàng\", \"Số điện thoại\", \"Địa Chỉ\", \"Tình trạng đồng hồ\"])\n",
    "for link in links:\n",
    "    print (link)\n",
    "    writer.writerow(get_info_from_link(link))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "for year in years:\n",
    "    links = get_link_by_year(year)\n",
    "    \n",
    "    for link in links:\n",
    "        #print(link)\n",
    "        f = open(link[1], 'w', newline='', encoding=\"utf-8\")\n",
    "        writer = csv.writer(f, delimiter=\"\\t\")\n",
    "\n",
    "        soup = soup_site(link[0])\n",
    "\n",
    "\n",
    "        if soup != -1:\n",
    "        #     temp = soup.findAll(\"h2\")[0]\n",
    "        #     print(temp.findNext(\"ul\", {\"class\" : \"publ-list\"}))\n",
    "\n",
    "            #year = soup.findAll(\"h2\")[0].text[-4:]\n",
    "            #year = soup.findAll(\"h1\")[0].text[8:13]\n",
    "            #print(year)\n",
    "\n",
    "            sections = soup.findAll(\"ul\", {\"class\" : \"publ-list\"})\n",
    "            secddd = soup.findAll(\"doi\")\n",
    "            #print(type(sections)) \n",
    "            #print(len(sections))\n",
    "            #print(sections[0])\n",
    "            for section in sections:\n",
    "                papers = section.findAll(\"cite\", {\"class\" : \"data\"})\n",
    "                #print(type(papers)) \n",
    "                #print(len(papers))\n",
    "                #print(papers)\n",
    "                for paper in papers:\n",
    "                    authors = [t.text for t in paper.findAll(\"span\", {\"itemprop\" : \"author\"})]\n",
    "                    title = paper.find(\"span\", {\"class\" : \"title\"}).text\n",
    "                    #print(authors, title, years)\n",
    "                    writer.writerow([\" \".join(authors), title, year])\n",
    "\n",
    "        f.close()\n",
    "        print(link[0] + \" Done\")    \n",
    "print(\"FINAL Done\") \n",
    "#    print(paper)\n",
    "#    doi = paper.find(\"div\", {\"class\" : \"head\"}).find(\"a\")['href']\n",
    "#    #print(doi)\n",
    "#    data = paper.find(\"div\", {\"class\" : \"data\"})\n",
    "#    if data is None:\n",
    "#        data = paper.find(\"article\", {\"class\" : \"data\"})\n",
    "#    authors = [t.text for t in data.findAll(\"span\", {\"itemprop\" : \"author\"})]\n",
    "#    title = data.find(\"span\", {\"class\" : \"title\"}).text\n",
    "    \n",
    "#     title = soup.find(\"li\", {\"class\" : \"entry article\"}).find(\"div\", {\"class\" : \"data\"}).find(\"span\", {\"class\" : \"title\"}).text\n",
    "#     authors = [t.text for t in soup.find(\"li\", {\"class\" : \"entry article\"}).find(\"div\", {\"class\" : \"data\"}).findAll(\"span\", {\"itemprop\" : \"author\"})]\n",
    "#     year = soup.findAll(\"header\")[1].text[-4:]\n",
    "#     doi = soup.find(\"li\", {\"class\" : \"entry article\"}).find(\"div\", {\"class\" : \"head\"}).find(\"a\")['href']\n",
    "#    print(authors, title, year, doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https://dblp.org/db/conf/ccs/ccs2019.html', 'ccs2019.csv'), ('https://dblp.org/db/conf/uss/uss2019.html', 'uss2019.csv'), ('https://dblp.org/db/conf/sp/sp2019.html', 'sp2019.csv'), ('https://dblp.org/db/conf/ndss/ndss2019.html', 'ndss2019.csv')]\n",
      "[('https://dblp.org/db/conf/ccs/ccs2018.html', 'ccs2018.csv'), ('https://dblp.org/db/conf/uss/uss2018.html', 'uss2018.csv'), ('https://dblp.org/db/conf/sp/sp2018.html', 'sp2018.csv'), ('https://dblp.org/db/conf/ndss/ndss2018.html', 'ndss2018.csv')]\n",
      "[('https://dblp.org/db/conf/ccs/ccs2017.html', 'ccs2017.csv'), ('https://dblp.org/db/conf/uss/uss2017.html', 'uss2017.csv'), ('https://dblp.org/db/conf/sp/sp2017.html', 'sp2017.csv'), ('https://dblp.org/db/conf/ndss/ndss2017.html', 'ndss2017.csv')]\n",
      "[('https://dblp.org/db/conf/ccs/ccs2016.html', 'ccs2016.csv'), ('https://dblp.org/db/conf/uss/uss2016.html', 'uss2016.csv'), ('https://dblp.org/db/conf/sp/sp2016.html', 'sp2016.csv'), ('https://dblp.org/db/conf/ndss/ndss2016.html', 'ndss2016.csv')]\n",
      "[('https://dblp.org/db/conf/ccs/ccs2015.html', 'ccs2015.csv'), ('https://dblp.org/db/conf/uss/uss2015.html', 'uss2015.csv'), ('https://dblp.org/db/conf/sp/sp2015.html', 'sp2015.csv'), ('https://dblp.org/db/conf/ndss/ndss2015.html', 'ndss2015.csv')]\n"
     ]
    }
   ],
   "source": [
    "years = range (2019,2014, -1)\n",
    "#print (list(years))\n",
    "\n",
    "for year in years:\n",
    "    test = get_link_by_year(year)\n",
    "    print (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NavigableString' object has no attribute 'tr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-5e7c4f148b68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#print (table_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtable_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtr\u001b[0m             \u001b[1;31m# get all contents of <tr> tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    644\u001b[0m             raise AttributeError(\n\u001b[0;32m    645\u001b[0m                 \"'%s' object has no attribute '%s'\" % (\n\u001b[1;32m--> 646\u001b[1;33m                     self.__class__.__name__, attr))\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moutput_ready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"minimal\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NavigableString' object has no attribute 'tr'"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "if soup != -1:\n",
    "\n",
    "    information = soup.findAll(\"div\", {\"class\" : \"info_label\"})\n",
    "    table_data = soup.table\n",
    "    #print (table_data)\n",
    "    for table in table_data:\n",
    "        fields = table.tr             # get all contents of <tr> tag\n",
    "        for item in fields:\n",
    "            temp = item.td.string\n",
    "            print (\" {0}\".format(temp))\n",
    "            \n",
    "    for info in information:\n",
    "        field = info.label            # get all contents of <table> tag\n",
    "        data_temp = info.b            # get all contents of <b> tag\n",
    "        data_temp = data_temp.string  # get payload of contents\n",
    "        print (field.string, data_temp)\n",
    "        data.append(data_temp)\n",
    "    \n",
    "    print( data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    links = get_link_by_year(year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://dblp.org/db/conf/ccs/ccs2019.html Done\n",
      "https://dblp.org/db/conf/uss/uss2019.html Done\n",
      "https://dblp.org/db/conf/sp/sp2019.html Done\n",
      "https://dblp.org/db/conf/ndss/ndss2019.html Done\n",
      "https://dblp.org/db/conf/ccs/ccs2018.html Done\n",
      "https://dblp.org/db/conf/uss/uss2018.html Done\n",
      "https://dblp.org/db/conf/sp/sp2018.html Done\n",
      "https://dblp.org/db/conf/ndss/ndss2018.html Done\n",
      "https://dblp.org/db/conf/ccs/ccs2017.html Done\n",
      "https://dblp.org/db/conf/uss/uss2017.html Done\n",
      "https://dblp.org/db/conf/sp/sp2017.html Done\n",
      "https://dblp.org/db/conf/ndss/ndss2017.html Done\n",
      "https://dblp.org/db/conf/ccs/ccs2016.html Done\n",
      "https://dblp.org/db/conf/uss/uss2016.html Done\n",
      "https://dblp.org/db/conf/sp/sp2016.html Done\n",
      "https://dblp.org/db/conf/ndss/ndss2016.html Done\n",
      "https://dblp.org/db/conf/ccs/ccs2015.html Done\n",
      "https://dblp.org/db/conf/uss/uss2015.html Done\n",
      "https://dblp.org/db/conf/sp/sp2015.html Done\n",
      "https://dblp.org/db/conf/ndss/ndss2015.html Done\n",
      "FINAL Done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "for year in years:\n",
    "    links = get_link_by_year(year)\n",
    "    \n",
    "    for link in links:\n",
    "        #print(link)\n",
    "        f = open(link[1], 'w', newline='', encoding=\"utf-8\")\n",
    "        writer = csv.writer(f, delimiter=\"\\t\")\n",
    "\n",
    "        soup = soup_site(link[0])\n",
    "\n",
    "\n",
    "        if soup != -1:\n",
    "        #     temp = soup.findAll(\"h2\")[0]\n",
    "        #     print(temp.findNext(\"ul\", {\"class\" : \"publ-list\"}))\n",
    "\n",
    "            #year = soup.findAll(\"h2\")[0].text[-4:]\n",
    "            #year = soup.findAll(\"h1\")[0].text[8:13]\n",
    "            #print(year)\n",
    "\n",
    "            sections = soup.findAll(\"ul\", {\"class\" : \"publ-list\"})\n",
    "            secddd = soup.findAll(\"doi\")\n",
    "            #print(type(sections)) \n",
    "            #print(len(sections))\n",
    "            #print(sections[0])\n",
    "            for section in sections:\n",
    "                papers = section.findAll(\"cite\", {\"class\" : \"data\"})\n",
    "                #print(type(papers)) \n",
    "                #print(len(papers))\n",
    "                #print(papers)\n",
    "                for paper in papers:\n",
    "                    authors = [t.text for t in paper.findAll(\"span\", {\"itemprop\" : \"author\"})]\n",
    "                    title = paper.find(\"span\", {\"class\" : \"title\"}).text\n",
    "                    #print(authors, title, years)\n",
    "                    writer.writerow([\" \".join(authors), title, year])\n",
    "\n",
    "        f.close()\n",
    "        print(link[0] + \" Done\")    \n",
    "print(\"FINAL Done\") \n",
    "#    print(paper)\n",
    "#    doi = paper.find(\"div\", {\"class\" : \"head\"}).find(\"a\")['href']\n",
    "#    #print(doi)\n",
    "#    data = paper.find(\"div\", {\"class\" : \"data\"})\n",
    "#    if data is None:\n",
    "#        data = paper.find(\"article\", {\"class\" : \"data\"})\n",
    "#    authors = [t.text for t in data.findAll(\"span\", {\"itemprop\" : \"author\"})]\n",
    "#    title = data.find(\"span\", {\"class\" : \"title\"}).text\n",
    "    \n",
    "#     title = soup.find(\"li\", {\"class\" : \"entry article\"}).find(\"div\", {\"class\" : \"data\"}).find(\"span\", {\"class\" : \"title\"}).text\n",
    "#     authors = [t.text for t in soup.find(\"li\", {\"class\" : \"entry article\"}).find(\"div\", {\"class\" : \"data\"}).findAll(\"span\", {\"itemprop\" : \"author\"})]\n",
    "#     year = soup.findAll(\"header\")[1].text[-4:]\n",
    "#     doi = soup.find(\"li\", {\"class\" : \"entry article\"}).find(\"div\", {\"class\" : \"head\"}).find(\"a\")['href']\n",
    "#    print(authors, title, year, doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup = soup_site(\"http://dblp.uni-trier.de/db/journals/isci/isci{0}.html\".format(254))\n",
    "# soup = soup_site(\"http://dblp.uni-trier.de/db/journals/dss/dss{0}.html\".format(52))\n",
    "# soup = soup_site(\"http://dblp.uni-trier.de/db/journals/tist/tist{0}.html\".format(9))\n",
    "# soup = soup_site(\"http://dblp.uni-trier.de/db/journals/tkde/tkde{0}.html\".format(22))\n",
    "#soup = soup_site(\"http://dblp.uni-trier.de/db/journals/inffus/inffus{0}.html\".format(42))\n",
    "soup = soup_site(venues_lists[0])\n",
    "#top 5 journals\n",
    "#1. ACM Symposium on Computer and Communications Security\t82\t123\n",
    "#2. USENIX Security Symposium\t81\t116\n",
    "#3. IEEE Transactions on Information Forensics and Security\t78\t106\n",
    "#4. IEEE Symposium on Security and Privacy\t72\t128\n",
    "#. Network and Distributed System Security Symposium (NDSS)\t65\t112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dblp.org/db/journals/pvldb/pvldb12.html\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0034d754cca5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpaper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"article\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"data\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mauthors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"span\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"itemprop\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"author\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"span\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"title\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mauthors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# For PVLDB only\n",
    "f = open('pvldb.csv', 'w', newline='')\n",
    "\n",
    "writer = csv.writer(f, delimiter=\"\\t\")\n",
    "\n",
    "# for i in range(15, 41):\n",
    "# for i in range(1, 38+1):\n",
    "# for i in range(4, 12+1):\n",
    "# for i in range(23, 30+1):\n",
    "# for i in range(42, 124+1):\n",
    "# for i in range(42, 50+1):\n",
    "# for i in range(1, 12+1):\n",
    "for i in range(12, 12+1):\n",
    "    volume = \"http://dblp.org/db/journals/pvldb/pvldb{}.html\".format(i)\n",
    "    soup = soup_site(volume)\n",
    "    if soup == -1:\n",
    "        continue\n",
    "    print(volume)\n",
    "    \n",
    "    h2sections = soup.findAll(\"h2\")\n",
    "    for h2section in soup.findAll(\"h2\"):\n",
    "        year = h2section.text[-4:]\n",
    "        section = h2section.findNext(\"ul\", {\"class\" : \"publ-list\"})\n",
    "        for paper in section.findAll(\"li\", {\"class\" : \"entry article\"}):\n",
    "            doi = paper.find(\"div\", {\"class\" : \"head\"}).find(\"a\")['href']\n",
    "            data = paper.find(\"div\", {\"class\" : \"data\"})\n",
    "            if data is None:\n",
    "                data = paper.find(\"article\", {\"class\" : \"data\"})\n",
    "            authors = [t.text for t in data.findAll(\"span\", {\"itemprop\" : \"author\"})]\n",
    "            title = data.find(\"span\", {\"class\" : \"title\"}).text\n",
    "            writer.writerow([\", \".join(authors), title, year, doi])\n",
    "#         print (authors, title, year)\n",
    "\n",
    "f.close()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dblp.uni-trier.de/db/journals/tkde/tkde31.html\n",
      "['Meichun Hsu', 'Alfons Kemper', 'Timos Sellis'] b'Special Section on the International Conference on Data Engineering 2016.' 2019\n",
      "['Jongwuk Lee', 'Won-Seok Hwang', 'Juan Parc', 'Youngnam Lee', 'Sang-Wook Kim', 'Dongwon Lee'] b'l-Injection: Toward Effective Collaborative Filtering Using Uninteresting Items.' 2019\n",
      "['Srinivas Karthik', 'Jayant R. Haritsa', 'Sreyash Kenkre', 'Vinayaka Pandit', 'Lohit Krishnan'] b'Platform-Independent Robust Query Processing.' 2019\n",
      "['Li Su', 'Yongluan Zhou'] b'Passive and Partially Active Fault Tolerance for Massively Parallel Stream Processing Engines.' 2019\n",
      "['Manas Joglekar', 'Hector Garcia-Molina', 'Aditya G. Parameswaran'] b'Interactive Data Exploration with Smart Drill-Down.' 2019\n",
      "['Pinghui Wang', 'Yiyan Qi', 'John C. S. Lui', 'Don Towsley', 'Junzhou Zhao', 'Jing Tao'] b'Inferring Higher-Order Structure Statistics of Large Networks from Sampled Edges.' 2019\n",
      "['Dong Wen', 'Lu Qin', 'Ying Zhang', 'Xuemin Lin', 'Jeffrey Xu Yu'] b'I/O Efficient Core Graph Decomposition: Application to Degeneracy Ordering.' 2019\n",
      "['Zimeng Zhou', 'Chenyun Yu', 'Sarana Nutanong', 'Yufei Cui', 'Chenchen Fu', 'Chun Jason Xue'] b'A Hardware-Accelerated Solution for Hierarchical Index-Based Merge-Join.' 2019\n",
      "['Chaoyue Niu', 'Zhenzhe Zheng', 'Fan Wu', 'Xiaofeng Gao', 'Guihai Chen'] b'Achieving Data Truthfulness and Privacy Preservation in Data Markets.' 2019\n",
      "['Bing Li', 'Xiaochun Yang', 'Rui Zhou', 'Bin Wang', 'Chengfei Liu', 'Yanchun Zhang'] b'An Efficient Method for High Quality and Cohesive Topical Phrase Mining.' 2019\n",
      "['Xu Zhou', 'Kenli Li', 'ZhiBang Yang', 'Keqin Li'] b'Finding Optimal Skyline Product Combinations under Price Promotion.' 2019\n",
      "['Qi Zhang', 'Chongyang Shi', 'Zhendong Niu', 'Longbing Cao'] b'HCBC: A Hierarchical Case-Based Classifier Integrated with Conceptual Clustering.' 2019\n",
      "['Qian Ma', 'Yu Gu', 'Wang-Chien Lee', 'Ge Yu'] b'Order-Sensitive Imputation for Clustered Missing Values.' 2019\n",
      "['Konstantinos Semertzidis', 'Evaggelia Pitoura'] b'Top-k Durable Graph Pattern Queries on Temporal Graphs.' 2019\n",
      "['Jerónimo Hernández-González', 'Iñaki Inza', 'José Antonio Lozano'] b'A Note on the Behavior of Majority Voting in Multi-Class Domains with Biased Annotators.' 2019\n",
      "['Liangxiao Jiang', 'Lungan Zhang', 'Chaoqun Li', 'Jia Wu'] b'A Correlation-Based Feature Weighting Filter for Naive Bayes.' 2019\n",
      "['Peilin Zhao', 'Yifan Zhang', 'Min Wu', 'Steven C. H. Hoi', 'Mingkui Tan', 'Junzhou Huang'] b'Adaptive Cost-Sensitive Online Classification.' 2019\n",
      "['Amirhossein Akbarnejad', 'Mahdieh Soleymani Baghshah'] b'An Efficient Semi-Supervised Multi-label Classifier Capable of Handling Missing Labels.' 2019\n",
      "['Zeyang Ye', 'Keli Xiao', 'Yong Ge', 'Yuefan Deng'] b'Applying Simulated Annealing and Parallel Computing to the Mobile Sequential Recommendation.' 2019\n",
      "['Shangsong Liang', 'Emine Yilmaz', 'Evangelos Kanoulas'] b'Collaboratively Tracking Interests for User Clustering in Streams of Short Texts.' 2019\n",
      "['Xiaoke Ma', 'Di Dong', 'Quan Wang'] b'Community Detection in Multi-Layer Networks Using Joint Nonnegative Matrix Factorization.' 2019\n",
      "['Yifan Chen', 'Xiang Zhao', 'Xuemin Lin', 'Yang Wang', 'Deke Guo'] b'Efficient Mining of Frequent Patterns on Uncertain Graphs.' 2019\n",
      "['Tin C. Truong', 'Hai V. Duong', 'Bac Le', 'Philippe Fournier-Viger'] b'Efficient Vertical Mining of High Average-Utility Itemsets Based on Novel Upper-Bounds.' 2019\n",
      "['Georgios Kellaris', 'Stavros Papadopoulos', 'Dimitris Papadias'] b'Engineering Methods for Differentially Private Histograms: Efficiency Beyond Utility.' 2019\n",
      "['Sepehr Eghbali', 'Ladan Tahvildari'] b'Fast Cosine Similarity Search in Binary Space with Angular Multi-Index Hashing.' 2019\n",
      "['Linlin You', 'Bige Tunçer', 'Hexu Xing'] b'Harnessing Multi-Source Data about Public Sentiments and Activities for Informed Design.' 2019\n",
      "['Chuan Shi', 'Binbin Hu', 'Wayne Xin Zhao', 'Philip S. Yu'] b'Heterogeneous Information Network Embedding for Recommendation.' 2019\n",
      "['Dimitrios Kotzias', 'Moshe Lichman', 'Padhraic Smyth'] b'Predicting Consumption Patterns with Repeated and Novel Events.' 2019\n",
      "['Bingqing Lyu', 'Lu Qin', 'Xuemin Lin', 'Lijun Chang', 'Jeffrey Xu Yu'] b'Supergraph Search in Graph Databases via Hierarchical Feature-Tree.' 2019\n",
      "['Marco Ferrante', 'Nicola Ferro', 'Silvia Pontarollo'] b'A General Theory of IR Evaluation Measures.' 2019\n",
      "['Hangyu Li', 'Sarana Nutanong', 'Hong Xu', 'Chenyun Yu', 'Foryu Ha'] b'C2Net: A Network-Efficient Approach to Collision Counting LSH Similarity Join.' 2019\n",
      "['Inbal Yahav', 'Onn Shehory', 'David G. Schwartz'] b'Comments Mining With TF-IDF: The Inherent Bias and Its Removal.' 2019\n",
      "['Yuan He', 'Cheng Wang', 'Changjun Jiang'] b'Correlated Matrix Factorization for Recommendation with Implicit Feedback.' 2019\n",
      "['Bowen Du', 'Chuanren Liu', 'Wenjun Zhou', 'Zhenshan Hou', 'Hui Xiong'] b'Detecting Pickpocket Suspects from Large-Scale Public Transit Records.' 2019\n",
      "['Hong Cao', 'Kevin Chen-Chuan Chang'] b'Nonintrusive Smartphone User Verification Using Anonymized Multimodal Data.' 2019\n",
      "['Jiali Mei', 'Yohann de Castro', 'Yannig Goude', 'Jean-Marc Azaïs', 'Georges Hébrail'] b'Nonnegative Matrix Factorization with Side Information for Time Series Recovery and Prediction.' 2019\n",
      "['Dingqi Yang', 'Bingqing Qu', 'Philippe Cudré-Mauroux'] b'Privacy-Preserving Social Media Data Publishing for Personalized Ranking-Based Recommendation.' 2019\n",
      "['Xu Zhou', 'Kenli Li', 'ZhiBang Yang', 'Guoqing Xiao', 'Keqin Li'] b'Progressive Approaches for Pareto Optimal Groups Computation.' 2019\n",
      "['Yanjie Fu', 'Guannan Liu', 'Yong Ge', 'Pengyang Wang', 'Hengshu Zhu', 'Chunxiao Li', 'Hui Xiong'] b'Representing Urban Forms: A Collective Learning Model with Heterogeneous Human Mobility Data.' 2019\n",
      "['Zhenjun Tang', 'Lv Chen', 'Xianquan Zhang', 'Shichao Zhang'] b'Robust Image Hashing with Tensor Decomposition.' 2019\n",
      "['Wenhao Jiang', 'Hongchang Gao', 'Wei Lu', 'Wei Liu', 'Fu-Lai Chung', 'Heng Huang'] b'Stacked Robust Adaptively Regularized Auto-Regressions for Domain Adaptation.' 2019\n",
      "['Houping Xiao', 'Jing Gao', 'Qi Li', 'Fenglong Ma', 'Lu Su', 'Yunlong Feng', 'Aidong Zhang'] b'Towards Confidence Interval Estimation in Truth Discovery.' 2019\n",
      "['Arman Sepehr', 'Hamid Beigy'] b'Viral Cascade Probability Estimation and Maximization in Diffusion Networks.' 2019\n",
      "['Chong Wang', 'Shuai Zhao', 'Achir Kalra', 'Cristian Borcea', 'Yi Chen'] b'Webpage Depth Viewability Prediction Using Deep Sequential Neural Networks.' 2019\n",
      "['Majid Mohammadi', 'Wout Hofman', 'Yao-Hua Tan'] b'A Comparative Study of Ontology Matching Systems via Inferential Statistics.' 2019\n",
      "['Haifeng Zhao', 'Zheng Wang', 'Feiping Nie'] b'A New Formulation of Linear Discriminant Analysis for Robust Dimensionality Reduction.' 2019\n",
      "['Punit Rathore', 'Dheeraj Kumar', 'James C. Bezdek', 'Sutharshan Rajasegarar', 'Marimuthu Palaniswami'] b'A Rapid Hybrid Clustering Algorithm for Large Volumes of High Dimensional Data.' 2019\n",
      "['Yiwen Nie', 'Wei Yang', 'Liusheng Huang', 'Xike Xie', 'Zhenhua Zhao', 'Shaowei Wang'] b'A Utility-Optimized Framework for Personalized Private Histogram Estimation.' 2019\n",
      "['Peng Peng', 'Lei Zou', 'Lei Chen', 'Dongyan Zhao'] b'Adaptive Distributed RDF Graph Fragmentation and Allocation based on Query Workload.' 2019\n",
      "['Adriano Augusto', 'Raffaele Conforti', 'Marlon Dumas', 'Marcello La Rosa', 'Fabrizio Maria Maggi', 'Andrea Marrella', 'Massimo Mecella', 'Allar Soo'] b'Automated Discovery of Process Models from Event Logs: Review and Benchmark.' 2019\n",
      "['Ting Guo', 'Shirui Pan', 'Xingquan Zhu', 'Chengqi Zhang'] b'CFOND: Consensus Factorization for Co-Clustering Networked Data.' 2019\n",
      "['Clemens Blöchl', 'Rana Ali Amjad', 'Bernhard C. Geiger'] b'Co-Clustering via Information-Theoretic Markov Aggregation.' 2019\n",
      "['Wei Lu', 'Yanyan Shen', 'Tongtong Wang', 'Meihui Zhang', 'H. V. Jagadish', 'Xiaoyong Du'] b'Fast Failure Recovery in Vertex-Centric Distributed Graph Processing Systems.' 2019\n",
      "['Fang Liu'] b'Generalized Gaussian Mechanism for Differential Privacy.' 2019\n",
      "['Yang Yang', 'Yaqian Duan', 'Xinze Wang', 'Zi Huang', 'Ning Xie', 'Heng Tao Shen'] b'Hierarchical Multi-Clue Modelling for POI Popularity Prediction with Heterogeneous Tourist Information.' 2019\n",
      "['Yongquan Dong', 'Eduard C. Dragut', 'Weiyi Meng'] b'Normalization of Duplicate Records from Multiple Sources.' 2019\n",
      "['Yixiang Fang', 'Zheng Wang', 'Reynold Cheng', 'Xiaodong Li', 'Siqiang Luo', 'Jiafeng Hu', 'Xiaojun Chen'] b'On Spatial-Aware Community Search.' 2019\n",
      "['Hongfu Liu', 'Ming Shao', 'Zhengming Ding', 'Yun Fu'] b'Structure-Preserved Unsupervised Domain Adaptation.' 2019\n",
      "['Takao Noguchi', 'Norman E. Fenton', 'Martin Neil'] b'Addressing the Practical Limitations of Noisy-OR Using Conditional Inter-Causal Anti-Correlation with Ranked Nodes.' 2019\n",
      "['Ting Bai', 'Wanye Xin Zhao', 'Yulan He', 'Jian-Yun Nie', 'Ji-Rong Wen'] b'Correction to \"Characterizing and Predicting Early Reviewers for Effective Product Marketing on E-Commerce Websites\".' 2019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lorenzo Baldacci', 'Matteo Golfarelli'] b'A Cost Model for SPARK SQL.' 2019\n",
      "['Peng Cui', 'Xiao Wang', 'Jian Pei', 'Wenwu Zhu'] b'A Survey on Network Embedding.' 2019\n",
      "['Songlei Jian', 'Guansong Pang', 'Longbing Cao', 'Kai Lu', 'Hang Gao'] b'CURE: Flexible Categorical Data Representation by Hierarchical Coupling Learning.' 2019\n",
      "['Libin Zheng', 'Lei Chen'] b'DLTA: A Framework for Dynamic Crowdsourcing Classification Tasks.' 2019\n",
      "['Tianji Pang', 'Feiping Nie', 'Junwei Han', 'Xuelong Li'] b'Efficient Feature Selection via $\\\\ell _{2, 0}$\\xe2\\x84\\x932, 0-norm Constrained Sparse Regression.' 2019\n",
      "['Peng Cheng', 'Xiang Lian', 'Xun Jian', 'Lei Chen'] b'FROG: A Fast and Reliable Crowdsourcing Framework.' 2019\n",
      "['Wenkai Jiang', 'Jianzhong Qi', 'Jeffrey Xu Yu', 'Jin Huang', 'Rui Zhang'] b'HyperX: A Scalable Hypergraph Framework.' 2019\n",
      "['Zhiling Luo', 'Ling Liu', 'Jianwei Yin', 'Ying Li', 'Zhaohui Wu'] b'Latent Ability Model: A Generative Probabilistic Learning Framework for Workforce Analytics.' 2019\n",
      "['Xishun Wang', 'Minjie Zhang', 'Fenghui Ren'] b'Learning Customer Behaviors for Effective Load Forecasting.' 2019\n",
      "['Lailong Luo', 'Deke Guo', 'Xiang Zhao', 'Jie Wu', 'Ori Rottenstreich', 'Xueshan Luo'] b'Near-accurate Multiset Reconciliation.' 2019\n",
      "['Ranjan Pal', 'Pan Hui', 'Viktor K. Prasanna'] b'Privacy Engineering for the Smart Micro-Grid.' 2019\n",
      "['Sagar Sharma', 'James Powers', 'Keke Chen'] b'PrivateGraph: Privacy-Preserving Spectral Analysis of Encrypted Graphs in the Cloud.' 2019\n",
      "['Haoran Li', 'Junnan Zhu', 'Cong Ma', 'Jiajun Zhang', 'Chengqing Zong'] b'Read, Watch, Listen, and Summarize: Multi-Modal Summarization for Asynchronous Text, Image, Audio and Video.' 2019\n",
      "['Samaneh Aminikhanghahi', 'Tinghui Wang', 'Diane J. Cook'] b'Real-Time Change Point Detection with Application to Smart Home Time Series Data.' 2019\n",
      "['Donghui Hu', 'Dan Zhao', 'Shuli Zheng'] b'A New Robust Approach for Reversible Database Watermarking with Distortion Control.' 2019\n",
      "['Chunyu Hu', 'Yiqiang Chen', 'Xiaohui Peng', 'Han Yu', 'Chenlong Gao', 'Lisha Hu'] b'A Novel Feature Incremental Learning Method for Sensor-Based Activity Recognition.' 2019\n",
      "['Cunchao Tu', 'Xiangkai Zeng', 'Hao Wang', 'Zhengyan Zhang', 'Zhiyuan Liu', 'Maosong Sun', 'Bo Zhang', 'Leyu Lin'] b'A Unified Framework for Community Detection and Network Representation Learning.' 2019\n",
      "['Aneesh Sreevallabh Chivukula', 'Wei Liu'] b'Adversarial Deep Learning Models with Multiple Adversaries.' 2019\n",
      "['Min Peng', 'Qianqian Xie', 'Hua Wang', 'Yanchun Zhang', 'Gang Tian'] b'Bayesian Sparse Topical Coding.' 2019\n",
      "['Lei Shi', 'Zhiyang Teng', 'Le Wang', 'Yue Zhang', 'Alexander Binder'] b'DeepClue: Visual Interpretation of Text-Based Deep Stock Prediction.' 2019\n",
      "['Gergely Ács', 'Luca Melis', 'Claude Castelluccia', 'Emiliano De Cristofaro'] b'Differentially Private Mixture of Generative Neural Networks.' 2019\n",
      "['Tong Xu', 'Hengshu Zhu', 'Hao Zhong', 'Guannan Liu', 'Hui Xiong', 'Enhong Chen'] b'Exploiting the Dynamic Mutual Influence for Predicting Social Event Participation.' 2019\n",
      "['Yangjun Chen', 'Yibin Chen'] b'Introducing Cuts Into a Top-Down Process for Checking Tree Inclusion.' 2019\n",
      "['Pengfei Li', 'Hua Lu', 'Nattiya Kanhabua', 'Sha Zhao', 'Gang Pan'] b'Location Inference for Non-Geotagged Tweets in User Timelines.' 2019\n",
      "['Etienne Gael Tajeuna', 'Mohamed Bouguessa', 'Shengrui Wang'] b'Modeling and Predicting Community Structure Changes in Time-Evolving Social Networks.' 2019\n",
      "['Jun Xu', 'Wei Zeng', 'Yanyan Lan', 'Jiafeng Guo', 'Xueqi Cheng'] b'Modeling the Parameter Interactions in Ranking SVM with Low-Rank Approximation.' 2019\n",
      "['Shuo Shang', 'Lisi Chen', 'Kai Zheng', 'Christian S. Jensen', 'Zhewei Wei', 'Panos Kalnis'] b'Parallel Trajectory-to-Location Join.' 2019\n",
      "['Giovanni Simonini', 'George Papadakis', 'Themis Palpanas', 'Sonia Bergamaschi'] b'Schema-Agnostic Progressive Entity Resolution.' 2019\n",
      "['Wolfgang Gatterbauer', 'Arun Kumar'] b\"Guest Editors' Introduction to the Special Section on the 33rd International Conference on Data Engineering (ICDE 2017).\" 2019\n",
      "['Shangyu Luo', 'Zekai J. Gao', 'Michael N. Gubanov', 'Luis Leopoldo Perez', 'Christopher M. Jermaine'] b'Scalable Linear Algebra on a Relational Database System.' 2019\n",
      "['Son T. Mai', 'Sihem Amer-Yahia', 'Ira Assent', 'Mathias Skovgaard Birk', 'Martin Storgaard Dieu', 'Jon Jacobsen', 'Jesper Kristensen'] b'Scalable Interactive Dynamic Graph Clustering on Multicore CPUs.' 2019\n",
      "['Michail Vlachos', 'Celestine Dünner', 'Reinhard Heckel', 'Vassilios G. Vassiliadis', 'Thomas P. Parnell', 'Kubilay Atasu'] b'Addressing Interpretability and Cold-Start in Matrix Factorization for Recommender Systems.' 2019\n",
      "['Xing Niu', 'Raghav Kapoor', 'Boris Glavic', 'Dieter Gawlick', 'Zhen Hua Liu', 'Vasudha Krishnaswamy', 'Venkatesh Radhakrishnan'] b'Heuristic and Cost-Based Optimization for Diverse Provenance Tasks.' 2019\n",
      "['Yang Cao', 'Masatoshi Yoshikawa', 'Yonghui Xiao', 'Li Xiong'] b'Quantifying Differential Privacy in Continuous Data Release Under Temporal Correlations.' 2019\n",
      "['David Broneske', 'Veit Köppen', 'Gunter Saake', 'Martin Schäler'] b'Efficient Evaluation of Multi-Column Selection Predicates in Main-Memory.' 2019\n",
      "['Ju Fan', 'Zhewei Wei', 'Dongxiang Zhang', 'Jingru Yang', 'Xiaoyong Du'] b'Distribution-Aware Crowdsourced Entity Collection.' 2019\n",
      "['Yanhao Wang', 'Yuchen Li', 'Kian-Lee Tan'] b'Efficient Representative Subset Selection over Sliding Windows.' 2019\n",
      "['Xi Zhang', 'Yuan Su', 'Siyu Qu', 'Sihong Xie', 'Binxing Fang', 'Philip S. Yu'] b'IAD: Interaction-Aware Diffusion Framework in Social Networks.' 2019\n",
      "['Victor S. Sheng', 'Jing Zhang', 'Bin Gu', 'Xindong Wu'] b'Majority Voting and Pairing with Multiple Noisy Labeling.' 2019\n",
      "['Shao-Yuan Li', 'Yuan Jiang', 'Nitesh V. Chawla', 'Zhi-Hua Zhou'] b'Multi-Label Learning from Crowds.' 2019\n",
      "['Minh C. Phan', 'Aixin Sun', 'Yi Tay', 'Jialong Han', 'Chenliang Li'] b'Pair-Linking for Collective Entity Disambiguation: Two Could Be Better Than All.' 2019\n",
      "['Jinfei Liu', 'Juncheng Yang', 'Li Xiong', 'Jian Pei'] b'Secure and Efficient Skyline Queries on Encrypted Data.' 2019\n",
      "['Ke Deng', 'Yanhua Li', 'Jia Zeng', 'Mingxuan Yuan', 'Jun Luo', 'Jeffrey Xu Yu'] b'User Preference Analysis for Most Frequent Peer/Dominator.' 2019\n",
      "['Angelika Kimmig', 'Alex Memory', 'Renée J. Miller', 'Lise Getoor'] b'A Collective, Probabilistic Approach to Schema Mapping Using Diverse Noisy Evidence.' 2019\n",
      "['Pengfei Wei', 'Yiping Ke', 'Chi Keong Goh'] b'A General Domain Specific Feature Transfer Framework for Hybrid Domain Adaptation.' 2019\n",
      "['Massimo Melucci'] b'An Efficient Algorithm to Compute a Quantum Probability Space.' 2019\n",
      "['Liang Bai', 'Jiye Liang', 'Hangyuan Du', 'Yike Guo'] b'An Information-Theoretical Framework for Cluster Ensemble.' 2019\n",
      "['Yunshu Du', 'Assefaw H. Gebremedhin', 'Matthew E. Taylor'] b'Analysis of University Fitness Center Data Uncovers Interesting Patterns, Enables Prediction.' 2019\n",
      "['Chengyu Wang', 'Yan Fan', 'Xiaofeng He', 'Aoying Zhou'] b'Decoding Chinese User Generated Categories for Fine-Grained Knowledge Harvesting.' 2019\n",
      "['Jing Zhang', 'Ming Wu', 'Victor S. Sheng'] b'Ensemble Learning from Crowds.' 2019\n",
      "['Shuai Zheng', 'Chris Ding', 'Feiping Nie', 'Heng Huang'] b'Harmonic Mean Linear Discriminant Analysis.' 2019\n",
      "['Xiaofeng Zhu', 'Shichao Zhang', 'Yonggang Li', 'Jilian Zhang', 'Lifeng Yang', 'Yue Fang'] b'Low-Rank Sparse Subspace for Spectral Clustering.' 2019\n",
      "['Anuj Karpatne', 'Imme Ebert-Uphoff', 'Sai Ravela', 'Hassan Ali Babaie', 'Vipin Kumar'] b'Machine Learning for the Geosciences: Challenges and Opportunities.' 2019\n",
      "['Yi Cui', 'Di Xiao', 'Dmitri Loguinov'] b'On Efficient External-Memory Triangle Listing.' 2019\n",
      "['Qing Wang', 'Chunqiu Zeng', 'Wubai Zhou', 'Tao Li', 'S. S. Iyengar', 'Larisa Shwartz', 'Genady Ya. Grabarnik'] b'Online Interactive Collaborative Filtering Using Multi-Armed Bandit with Dependent Arms.' 2019\n",
      "['Bei Shi', 'Wai Lam'] b'Reader Comment Digest through Latent Event Facets and News Specificity.' 2019\n",
      "['Shuai Jiang', 'Kan Li', 'Richard Yi Da Xu'] b'Relative Pairwise Relationship Constrained Non-Negative Matrix Factorisation.' 2019\n",
      "['Yawei Zhao', 'Kai Xu', 'En Zhu', 'Xinwang Liu', 'Xinzhong Zhu', 'Jianping Yin'] b'Triangle Lasso for Simultaneous Clustering and Optimization in Graph Datasets.' 2019\n",
      "['Yankai Chen', 'Yixiang Fang', 'Reynold Cheng', 'Yun Li', 'Xiaojun Chen', 'Jie Zhang'] b'Exploring Communities in Large Profiled Graphs.' 2019\n",
      "['Ruqian Lu', 'Xiaolong Jin', 'Songmao Zhang', 'Meikang Qiu', 'Xindong Wu'] b'A Study on Big Knowledge and Its Engineering Issues.' 2019\n",
      "['Zhe Jiang'] b'A Survey on Spatial Prediction Methods.' 2019\n",
      "['Quanming Yao', 'James T. Kwok'] b'Accelerated and Inexact Soft-Impute for Large-Scale Matrix and Tensor Completion.' 2019\n",
      "['Si Zhang', 'Hanghang Tong'] b'Attributed Network Alignment: Problem Definitions and Fast Solutions.' 2019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zeyi Wen', 'Jiashuai Shi', 'Bingsheng He', 'Jian Chen', 'Yawen Chen'] b'Efficient Multi-Class Probabilistic SVMs on GPUs.' 2019\n",
      "['Yang Zhang', 'Victor S. Sheng'] b'Fog-enabled Event Processing Based on IoT Resource Models.' 2019\n",
      "['Yansong Zhang', 'Yu Zhang', 'Shan Wang', 'Jiaheng Lu'] b'Fusion OLAP: Fusing the Pros of MOLAP and ROLAP Together for In-Memory OLAP.' 2019\n",
      "['Meilian Lu', 'Zhenglin Zhang', 'Zhihe Qu', 'Yu Kang'] b'LPANNI: Overlapping Community Detection Using Label Propagation in Large-Scale Complex Networks.' 2019\n",
      "['Zisheng Zhang', 'Keshab K. Parhi'] b'MUSE: Minimum Uncertainty and Sample Elimination Based Binary Feature Selection.' 2019\n",
      "['Bowei Chen', 'Mohan S. Kankanhalli'] b'Pricing Average Price Advertising Options When Underlying Spot Market Prices Are Discontinuous.' 2019\n",
      "['Chenchen Fu', 'Qiangqiang Liu', 'Peng Wu', 'Minming Li', 'Chun Jason Xue', 'Yingchao Zhao', 'Jingtong Hu', 'Song Han'] b'Real-Time Data Retrieval in Cyber-Physical Systems with Temporal Validity and Data Availability Constraints.' 2019\n",
      "['Ioannis Mytilinis', 'Dimitrios Tsoumakos', 'Nectarios Koziris'] b'Scaling the Construction of Wavelet Synopses for Maximum Error Metrics.' 2019\n",
      "['Min Han', 'Shoubo Feng', 'C. L. Philip Chen', 'Meiling Xu', 'Tie Qiu'] b'Structured Manifold Broad Learning System: A Manifold Perspective for Large-Scale Chaotic Time Series Analysis and Prediction.' 2019\n",
      "['Peng Cheng', 'Ji Hu', 'Zidong Yang', 'Yuanchao Shu', 'Jiming Chen'] b'Utilization-Aware Trip Advisor in Bike-Sharing Systems Based on User Behavior Analysis.' 2019\n",
      "['Huimeng Wang', 'Yunyan Du', 'Jiawei Yi', 'Yong Sun', 'Fuyuan Liang'] b'A New Method for Measuring Topological Structure Similarity between Complex Trajectories.' 2019\n",
      "['Nannan Wu', 'Feng Chen', 'Jianxin Li', 'Jinpeng Huai', 'Baojian Zhou', 'Bo Li', 'Naren Ramakrishnan'] b'A Nonparametric Approach to Uncovering Connected Anomalies by Tree Shaped Priors.' 2019\n",
      "['Yingming Li', 'Ming Yang', 'Zhongfei Zhang'] b'A Survey of Multi-View Representation Learning.' 2019\n",
      "['Weihua Li', 'Quan Bai', 'Minjie Zhang', 'Tung Doan Nguyen'] b'Automated Influence Maintenance in Social Networks: An Agent-based Approach.' 2019\n",
      "['Dingqi Yang', 'Bin Li', 'Laura Rettig', 'Philippe Cudré-Mauroux'] b'D22HistoSketch: Discriminative and Dynamic Similarity-Preserving Sketching of Streaming Histograms.' 2019\n",
      "['Pinghui Wang', 'Peng Jia', 'Jing Tao', 'Xiaohong Guan'] b'Detecting a Variety of Long-Term Stealthy User Behaviors on High Speed Links.' 2019\n",
      "['Huang Xu', 'Zhiwen Yu', 'Jingyuan Yang', 'Hui Xiong', 'Hengshu Zhu'] b'Dynamic Talent Flow Analysis with Deep Sequence Prediction Modeling.' 2019\n",
      "['Ji Liu', 'Luis Pineda-Morales', 'Esther Pacitti', 'Alexandru Costan', 'Patrick Valduriez', 'Gabriel Antoniu', 'Marta Mattoso'] b'Efficient Scheduling of Scientific Workflows Using Hot Metadata in a Multisite Cloud.' 2019\n",
      "['Yu-Xuan Qiu', 'Rong-Hua Li', 'Jianxin Li', 'Shaojie Qiao', 'Guoren Wang', 'Jeffrey Xu Yu', 'Rui Mao'] b'Efficient Structural Clustering on Probabilistic Graphs.' 2019\n",
      "['Arun Iyengar'] b'Enhanced Clients for Data Stores and Cloud Services.' 2019\n",
      "['Kun Zhan', 'Chaoxi Niu', 'Changlu Chen', 'Feiping Nie', 'Changqing Zhang', 'Yi Yang'] b'Graph Structure Fusion for Multiview Clustering.' 2019\n",
      "['Aakas Zhiyuli', 'Xun Liang', 'YanFang Chen', 'Xiaoyong Du'] b'Modeling Large-Scale Dynamic Social Networks via Node Embeddings.' 2019\n",
      "['Vahid Jalili', 'Matteo Matteucci', 'Jeremy Goecks', 'Yashar Deldjoo', 'Stefano Ceri'] b'Next Generation Indexing for Genomic Intervals.' 2019\n",
      "['Xiaofeng Zhu', 'Shichao Zhang', 'Wei He', 'Rongyao Hu', 'Cong Lei', 'Pengfei Zhu'] b'One-Step Multi-View Spectral Clustering.' 2019\n",
      "['Lu Zhang', 'Yongkai Wu', 'Xintao Wu'] b'Causal Modeling-Based Discrimination Discovery and Removal: Criteria, Bounds, and Algorithms.' 2019\n",
      "['Xiongnan Jin', 'Sangjin Shin', 'Eunju Jo', 'Kyong-Ho Lee'] b'Collective Keyword Query on a Spatial Knowledge Base.' 2019\n",
      "['Shuyin Xia', 'Guoyin Wang', 'Zizhong Chen', 'Yanlin Duan', 'Qun Liu'] b'Complete Random Forest Based Class Noise Filtering Learning for Improving the Generalizability of Classifiers.' 2019\n",
      "['Zekai J. Gao', 'Niketan Pansare', 'Christopher M. Jermaine'] b'Declarative Parameterizations of User-Defined Functions for Large-Scale Machine Learning and Optimization.' 2019\n",
      "['Yixiang Fang', 'Zhongran Wang', 'Reynold Cheng', 'Hongzhi Wang', 'Jiafeng Hu'] b'Effective and Efficient Community Search Over Large Directed Graphs.' 2019\n",
      "['Huan Li', 'Hua Lu', 'Lidan Shou', 'Gang Chen', 'Ke Chen'] b'Finding Most Popular Indoor Semantic Locations Using Uncertain Mobility Data.' 2019\n",
      "['Hosein Azarbonyad', 'Mostafa Dehghani', 'Tom Kenter', 'Maarten Marx', 'Jaap Kamps', 'Maarten de Rijke'] b'HiTR: Hierarchical Topic Model Re-Estimation for Measuring Topical Diversity of Documents.' 2019\n",
      "['Jungeun Kim', 'Sungsu Lim', 'Jae-Gil Lee', 'Byung Lee Lee'] b'LinkBlackHole**: Robust Overlapping Community Detection Using Link Embedding.' 2019\n",
      "['Riccardo Guidotti', 'Giulio Rossetti', 'Luca Pappalardo', 'Fosca Giannotti', 'Dino Pedreschi'] b'Personalized Market Basket Prediction with Temporal Annotated Recurring Sequences.' 2019\n",
      "['Peter Christen', 'Thilina Ranbaduge', 'Dinusha Vatsalan', 'Rainer Schnell'] b'Precise and Fast Cryptanalysis for Bloom Filter Based Privacy-Preserving Record Linkage.' 2019\n",
      "['Vahid Ranjbar', 'Mostafa Salehi', 'Pegah Jandaghi', 'Mahdi Jalili'] b'QANet: Tensor Decomposition Approach for Query-Based Anomaly Detection in Heterogeneous Information Networks.' 2019\n",
      "['Qiyu Kang', 'Wee Peng Tay'] b'Sequential Multi-Class Labeling in Crowdsourcing.' 2019\n",
      "['Chi Harold Liu', 'Jie Xu', 'Jian Tang', 'Jon Crowcroft'] b'Social-Aware Sequential Modeling of User Interests: A Deep Learning Approach.' 2019\n",
      "['Min Du', 'Feifei Li'] b'Spell: Online Streaming Parsing of Large Unstructured System Logs.' 2019\n",
      "['Xianpeng Liang', 'Di Wu', 'De-Shuang Huang'] b'Image Co-Segmentation via Locally Biased Discriminative Clustering.' 2019\n",
      "['Yang Cao', 'Masatoshi Yoshikawa', 'Yonghui Xiao', 'Li Xiong'] b'Errata on \"Quantifying Differential Privacy in Continuous Data Release under Temporal Correlations\".' 2019\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# f = open('isci.csv', 'a', newline='')\n",
    "# f = open('inffus.csv', 'a', newline='')\n",
    "# f = open('dss.csv', 'a', newline='')\n",
    "# f = open('is.csv', 'a', newline='')\n",
    "# f = open('tist.csv', 'a', newline='')\n",
    "f = open('tkde.csv', 'a', newline='')\n",
    "# f = open('eswa.csv', 'w', newline='')\n",
    "# f = open('tlt.csv', 'w', newline='')\n",
    "# f = open('access.csv', 'w', newline='')\n",
    "# f = open('asc.csv', 'a', newline='')\n",
    "# f = open('ijitpm.csv', 'a', newline='')\n",
    "\n",
    "writer = csv.writer(f, delimiter=\"\\t\")\n",
    "\n",
    "# for i in range(15, 41):\n",
    "# for i in range(1, 38+1):\n",
    "# for i in range(4, 12+1):\n",
    "# for i in range(23, 30+1):\n",
    "# for i in range(42, 124+1):\n",
    "# for i in range(42, 50+1):\n",
    "# for i in range(1, 12+1):\n",
    "for i in range(31, 31+1):\n",
    "#     volume = \"http://dblp.uni-trier.de/db/journals/isci/isci{0}.html\".format(i)\n",
    "#     volume = \"http://dblp.uni-trier.de/db/journals/inffus/inffus{}.html\".format(i)\n",
    "#     volume = \"http://dblp.uni-trier.de/db/journals/dss/dss{}.html\".format(i)\n",
    "#     volume = \"http://dblp.uni-trier.de/db/journals/is/is{}.html\".format(i)\n",
    "#     volume = \"http://dblp.uni-trier.de/db/journals/tist/tist{}.html\".format(i)\n",
    "    volume = \"http://dblp.uni-trier.de/db/journals/tkde/tkde{}.html\".format(i)\n",
    "#     volume = \"http://dblp.uni-trier.de/db/journals/eswa/eswa{}.html\".format(i)\n",
    "#     volume = \"http://dblp.uni-trier.de/db/journals/tlt/tlt{}.html\".format(i)\n",
    "#     volume = \"http://dblp.uni-trier.de/db/journals/access/access{}.html\".format(i)\n",
    "#     volume = \"http://dblp.uni-trier.de/db/journals/asc/asc{}.html\".format(i)\n",
    "#     volume = \"http://dblp.uni-trier.de/db/journals/ijitpm/ijitpm{}.html\".format(i)\n",
    "    soup = soup_site(volume)\n",
    "    if soup == -1:\n",
    "        continue\n",
    "    print(volume)\n",
    "    \n",
    "    h2sections = soup.findAll(\"h2\")\n",
    "    year = h2sections[0].text[-4:]\n",
    "    for j, section in enumerate(soup.findAll(\"ul\", {\"class\" : \"publ-list\"})):\n",
    "        for paper in section.findAll(\"li\", {\"class\" : \"entry article\"}):\n",
    "            doi = paper.find(\"div\", {\"class\" : \"head\"}).find(\"a\")['href']\n",
    "            data = paper.find(\"div\", {\"class\" : \"data\"})\n",
    "            if data is None:\n",
    "                data = paper.find(\"article\", {\"class\" : \"data\"})\n",
    "            if data is None:\n",
    "                data = paper.find(\"cite\", {\"class\" : \"data\"})\n",
    "            authors = [t.text for t in data.findAll(\"span\", {\"itemprop\" : \"author\"})]\n",
    "            title = data.find(\"span\", {\"class\" : \"title\"}).text.encode(\"utf-8\")\n",
    "            writer.writerow([\", \".join(authors).encode(\"utf-8\"), title, year, doi])\n",
    "#             print (authors, title, year)\n",
    "\n",
    "f.close()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-21ec59bfd650>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"li\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"entry inproceedings\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"data\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"span\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"title\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"li\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"entry inproceedings\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"data\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"span\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"itemprop\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m\"author\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0myear\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# soup = soup_site(\"http://dblp.uni-trier.de/db/journals/isci/isci{0}.html\".format(254))\n",
    "# soup = soup_site(\"http://dblp.uni-trier.de/db/journals/dss/dss{0}.html\".format(52))\n",
    "# soup = soup_site(\"http://dblp.uni-trier.de/db/journals/tist/tist{0}.html\".format(9))\n",
    "# soup = soup_site(\"http://dblp.org/db/conf/www/www{}.html\".format(2018))\n",
    "soup = soup_site(\"http://dblp.org/db/conf/nips/nips{}.html\".format(2012))\n",
    "\n",
    "if soup != -1:\n",
    "    title = soup.find(\"li\", {\"class\" : \"entry inproceedings\"}).find(\"div\", {\"class\" : \"data\"}).find(\"span\", {\"class\" : \"title\"}).text\n",
    "    [t.text for t in soup.find(\"li\", {\"class\" : \"entry inproceedings\"}).find(\"div\", {\"class\" : \"data\"}).findAll(\"span\", {\"itemprop\" : \"author\"})]\n",
    "    year = soup.findAll(\"header\")[0].text\n",
    "    doi = soup.find(\"li\", {\"class\" : \"entry inproceedings\"}).find(\"div\", {\"class\" : \"head\"}).find(\"a\")['href']\n",
    "    print(doi)\n",
    "    print(authors, title, year, doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dblp.org/db/conf/icde/icde2018.html\n",
      "http://dblp.org/db/conf/icde/icde2019.html\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# f = open('www.csv', 'w', newline='')\n",
    "# f = open('sigmod.csv', 'w', newline='')\n",
    "# f = open('sigir.csv', 'w', newline='')\n",
    "# f = open('icde.csv', 'w', newline='')\n",
    "# f = open('cikm.csv', 'w', newline='')\n",
    "# f = open('icml.csv', 'w', newline='')\n",
    "# f = open('nips2.csv', 'w', newline='')\n",
    "# f = open('rita.csv', 'w', newline='')\n",
    "\n",
    "writer = csv.writer(f, delimiter=\"\\t\")\n",
    "\n",
    "# for i in range(2010, 2011+1):\n",
    "# for i in [2012,2013,2014,2015,2017,2018]:\n",
    "for i in [2018,2019]:\n",
    "#     volume = \"http://dblp.org/db/conf/www/www{}.html\".format(i)\n",
    "#     volume = \"http://dblp.org/db/conf/sigmod/sigmod{}.html\".format(i)\n",
    "#     volume = \"http://dblp.org/db/conf/sigir/sigir{}.html\".format(i)\n",
    "    volume = \"http://dblp.org/db/conf/icde/icde{}.html\".format(i)\n",
    "#     volume = \"http://dblp.org/db/conf/cikm/cikm{}.html\".format(i)\n",
    "#     volume = \"http://dblp.org/db/conf/icml/icml{}.html\".format(i)\n",
    "#     volume = \"http://dblp.org/db/conf/nips/nips{}.html\".format(i)\n",
    "#     volume = \"http://dblp.org/db/conf/rita/rita{}.html\".format(i)\n",
    "    soup = soup_site(volume)\n",
    "    if soup == -1:\n",
    "        continue\n",
    "    print(volume)\n",
    "    \n",
    "    year = i\n",
    "    \n",
    "    for paper in soup.findAll(\"li\", {\"class\" : \"entry inproceedings\"}):\n",
    "        try:\n",
    "            doi = paper.find(\"div\", {\"class\" : \"head\"}).find(\"a\")['href']\n",
    "        except:\n",
    "            doi = \"N/A\"\n",
    "            \n",
    "        data = paper.find(\"div\", {\"class\" : \"data\"})\n",
    "        if data is None:\n",
    "            data = paper.find(\"article\", {\"class\" : \"data\"})\n",
    "        authors = [t.text for t in data.findAll(\"span\", {\"itemprop\" : \"author\"})]\n",
    "        title = data.find(\"span\", {\"class\" : \"title\"}).text\n",
    "        writer.writerow([\", \".join(authors), title, year, doi])\n",
    "#         print(title, doi)\n",
    "#         print (authors, title, year)\n",
    "\n",
    "f.close()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3.7.4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "\n",
    "print(sys.version_info[0])\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
